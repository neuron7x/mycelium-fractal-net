{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction and Analysis\n",
    "\n",
    "This notebook demonstrates MyceliumFractalNet's feature extraction capabilities, showing how to:\n",
    "- Extract 18 standardized fractal features\n",
    "- Analyze feature distributions\n",
    "- Understand feature correlations\n",
    "- Use features for downstream ML tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if running in Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -q mycelium-fractal-net matplotlib numpy pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mycelium_fractal_net import (\n",
    "    make_simulation_config_demo,\n",
    "    run_mycelium_simulation_with_history,\n",
    "    compute_fractal_features,\n",
    ")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Features from a Simulation\n",
    "\n",
    "Run a simulation and extract all 18 standardized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "config = make_simulation_config_demo()\n",
    "result = run_mycelium_simulation_with_history(config)\n",
    "\n",
    "# Extract features\n",
    "features = compute_fractal_features(result)\n",
    "\n",
    "print(\"Extracted Features:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in features.items():\n",
    "    print(f\"  {key:20s}: {value:12.6f}\")\n",
    "\n",
    "print(f\"\\nTotal features: {len(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Categories\n",
    "\n",
    "The 18 features are organized into 4 categories:\n",
    "- **Geometric** (4): D_box, f_active, edge_density, cluster_coeff\n",
    "- **Statistical** (6): V_mean, V_std, V_skew, V_kurt, entropy, hurst\n",
    "- **Temporal** (4): dV_dt_mean, dV_dt_std, autocorr_lag1, persistence\n",
    "- **Structural** (4): gradient_mean, gradient_std, laplacian_mean, laplacian_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize features by category\n",
    "feature_categories = {\n",
    "    'Geometric': ['D_box', 'f_active', 'edge_density', 'cluster_coeff'],\n",
    "    'Statistical': ['V_mean', 'V_std', 'V_skew', 'V_kurt', 'entropy', 'hurst'],\n",
    "    'Temporal': ['dV_dt_mean', 'dV_dt_std', 'autocorr_lag1', 'persistence'],\n",
    "    'Structural': ['gradient_mean', 'gradient_std', 'laplacian_mean', 'laplacian_std']\n",
    "}\n",
    "\n",
    "# Display by category\n",
    "for category, feature_names in feature_categories.items():\n",
    "    print(f\"\\n{category} Features:\")\n",
    "    print(\"-\" * 50)\n",
    "    for name in feature_names:\n",
    "        if name in features:\n",
    "            print(f\"  {name:20s}: {features[name]:12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Distribution Analysis\n",
    "\n",
    "Generate multiple simulations to analyze feature distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple simulations with different seeds\n",
    "n_simulations = 50\n",
    "all_features = []\n",
    "\n",
    "print(f\"Running {n_simulations} simulations...\")\n",
    "for seed in range(n_simulations):\n",
    "    config_temp = make_simulation_config_demo()\n",
    "    config_temp.seed = seed\n",
    "    config_temp.steps = 50  # Shorter for speed\n",
    "    result_temp = run_mycelium_simulation_with_history(config_temp)\n",
    "    features_temp = compute_fractal_features(result_temp)\n",
    "    all_features.append(features_temp)\n",
    "    \n",
    "    if (seed + 1) % 10 == 0:\n",
    "        print(f\"  Completed {seed + 1}/{n_simulations}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df_features = pd.DataFrame(all_features)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(df_features)} feature vectors\")\n",
    "print(f\"  Shape: {df_features.shape}\")\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "print(df_features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions for key features\n",
    "key_features = ['D_box', 'V_mean', 'entropy', 'gradient_mean']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature_name in enumerate(key_features):\n",
    "    if feature_name in df_features.columns:\n",
    "        ax = axes[i]\n",
    "        df_features[feature_name].hist(bins=20, ax=ax, alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(df_features[feature_name].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {df_features[feature_name].mean():.3f}')\n",
    "        ax.set_xlabel(feature_name)\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title(f'Distribution of {feature_name}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Correlation Analysis\n",
    "\n",
    "Analyze correlations between different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "correlation_matrix = df_features.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.7:  # Threshold for high correlation\n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], \n",
    "                                   correlation_matrix.columns[j], \n",
    "                                   corr_value))\n",
    "\n",
    "print(\"\\nHighly Correlated Feature Pairs (|r| > 0.7):\")\n",
    "for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "    print(f\"  {feat1:20s} <-> {feat2:20s}: r = {corr:6.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Principal Component Analysis\n",
    "\n",
    "Use PCA to identify the most important feature dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(df_features)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Explained variance by component\n",
    "axes[0].bar(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "            pca.explained_variance_ratio_, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Explained Variance by Component')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "axes[1].plot(range(1, len(cumsum) + 1), cumsum, marker='o', linewidth=2)\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "print(f\"\\nNumber of components for 95% variance: {n_components_95}/{len(df_features.columns)}\")\n",
    "print(f\"Top 3 components explain {cumsum[2]:.1%} of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance for Classification\n",
    "\n",
    "Use a simple classifier to identify most discriminative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create synthetic binary labels based on fractal dimension threshold\n",
    "# (High complexity vs Low complexity patterns)\n",
    "threshold = df_features['D_box'].median()\n",
    "labels = (df_features['D_box'] > threshold).astype(int)\n",
    "\n",
    "print(f\"Created binary labels based on D_box threshold: {threshold:.3f}\")\n",
    "print(f\"  Class 0 (low complexity): {(labels == 0).sum()} samples\")\n",
    "print(f\"  Class 1 (high complexity): {(labels == 1).sum()} samples\")\n",
    "\n",
    "# Train random forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(df_features.drop('D_box', axis=1), labels)  # Don't use D_box itself\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': df_features.drop('D_box', axis=1).columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_importance.head(10)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], alpha=0.7, edgecolor='black')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 Features for Pattern Complexity Classification')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {row['feature']:20s}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Vector for ML Pipeline\n",
    "\n",
    "Show how to prepare features for downstream machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to standardized numpy array\n",
    "feature_array = df_features.values\n",
    "\n",
    "print(\"Feature Array Shape:\", feature_array.shape)\n",
    "print(f\"  {feature_array.shape[0]} samples\")\n",
    "print(f\"  {feature_array.shape[1]} features\")\n",
    "\n",
    "print(\"\\nFeature Vector Statistics:\")\n",
    "print(f\"  Mean: {feature_array.mean(axis=0)[:5]}...\")\n",
    "print(f\"  Std:  {feature_array.std(axis=0)[:5]}...\")\n",
    "\n",
    "# Example: Normalize for ML\n",
    "scaler = StandardScaler()\n",
    "feature_array_normalized = scaler.fit_transform(feature_array)\n",
    "\n",
    "print(\"\\nNormalized Feature Vector:\")\n",
    "print(f\"  Mean (should be ~0): {feature_array_normalized.mean(axis=0)[:5]}...\")\n",
    "print(f\"  Std (should be ~1):  {feature_array_normalized.std(axis=0)[:5]}...\")\n",
    "\n",
    "print(\"\\n✓ Features ready for ML pipeline\")\n",
    "print(\"  - Can be used for classification, regression, clustering\")\n",
    "print(\"  - Standard shape: (n_samples, 18)\")\n",
    "print(\"  - All features are numerical and well-scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ✓ Extraction of 18 standardized fractal features\n",
    "- ✓ Feature organization by category (Geometric, Statistical, Temporal, Structural)\n",
    "- ✓ Distribution analysis across multiple simulations\n",
    "- ✓ Feature correlation analysis\n",
    "- ✓ Dimensionality reduction with PCA\n",
    "- ✓ Feature importance for classification tasks\n",
    "- ✓ Preparation for ML pipelines\n",
    "\n",
    "For more exploration:\n",
    "- `01_field_simulation.ipynb` - Field simulation basics\n",
    "- `03_fractal_exploration.ipynb` - Deep dive into fractal dimensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
